# Plan de Reprise d'Activit√© (Disaster Recovery)
# Syst√®me RAG Enterprise

## Vue d'ensemble

Ce document d√©taille le plan de reprise d'activit√© (PRA) pour le syst√®me RAG Enterprise. Il couvre les proc√©dures de sauvegarde, restauration, basculement et r√©cup√©ration en cas de sinistre majeur.

## Objectifs de R√©cup√©ration

### RTO et RPO D√©finis

#### Recovery Time Objective (RTO)
- **Panne mineure** : < 15 minutes
- **Panne majeure** : < 2 heures
- **Sinistre complet** : < 8 heures

#### Recovery Point Objective (RPO)
- **Donn√©es transactionnelles** : < 5 minutes
- **Documents et embeddings** : < 1 heure
- **Configuration syst√®me** : < 24 heures

### Niveaux de Criticit√©

#### Critique (Tier 1)
- API Principal
- Base de donn√©es PostgreSQL
- Authentification Keycloak
- Cache Redis

#### Important (Tier 2)
- Worker Celery
- Base vectorielle Qdrant
- Stockage MinIO
- Monitoring Prometheus

#### Support (Tier 3)
- Grafana Dashboards
- ELK Stack
- Documentation
- Analytics

## Architecture de Haute Disponibilit√©

### Sites Principaux

#### Site Principal (Production)
- **Localisation** : R√©gion EU-West-1
- **Infrastructure** : Kubernetes cluster 3 n≈ìuds master + 6 workers
- **Stockage** : SSD persistants avec r√©plication
- **R√©seau** : Load balancer avec failover automatique

#### Site de Basculement (DR)
- **Localisation** : R√©gion EU-Central-1
- **Infrastructure** : Kubernetes cluster 3 n≈ìuds (warm standby)
- **Stockage** : R√©plication asynchrone du site principal
- **R√©seau** : DNS basculement automatique (TTL 60s)

#### Site de Sauvegarde (Cold)
- **Localisation** : R√©gion US-East-1
- **Infrastructure** : Images et configurations stock√©es
- **Stockage** : Sauvegardes quotidiennes chiffr√©es
- **D√©ploiement** : Automatis√© via Terraform/Ansible

### R√©plication des Donn√©es

#### Base de Donn√©es (PostgreSQL)
```bash
# Configuration de la r√©plication streaming
# Sur le serveur principal
echo "
# R√©plication
wal_level = replica
max_wal_senders = 3
max_replication_slots = 3
archive_mode = on
archive_command = 'aws s3 cp %p s3://rag-wal-backup/%f'
" >> /var/lib/postgresql/data/postgresql.conf

# Cr√©ation du slot de r√©plication
sudo -u postgres psql -c "SELECT pg_create_physical_replication_slot('dr_slot');"
```

#### Cache Redis
```bash
# Configuration de la r√©plication Redis
# redis.conf sur le serveur principal
save 900 1
save 300 10
save 60 10000
rdbcompression yes
rdbchecksum yes

# Configuration du r√©plica
kubectl apply -f - <<EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-replica-dr
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis-replica-dr
  template:
    metadata:
      labels:
        app: redis-replica-dr
    spec:
      containers:
      - name: redis
        image: redis:7-alpine
        command: ["redis-server"]
        args: ["--replicaof", "redis-master.rag-production.svc.cluster.local", "6379"]
        env:
        - name: REDIS_MASTER_PASSWORD
          valueFrom:
            secretKeyRef:
              name: redis-secrets
              key: password
EOF
```

#### Stockage Objet (MinIO)
```bash
# Configuration de la r√©plication MinIO
mc alias set minio-prod https://minio-prod.rag.com minioadmin password
mc alias set minio-dr https://minio-dr.rag.com minioadmin password

# R√©plication automatique
mc replicate add minio-prod/rag-documents minio-dr/rag-documents-replica \
  --replicate "delete,metadata"

# V√©rification de la r√©plication
mc replicate ls minio-prod/rag-documents
```

#### Base Vectorielle (Qdrant)
```bash
# Sauvegarde r√©guli√®re des collections
curl -X POST "http://qdrant-master:6333/collections/enterprise_rag/snapshots" \
  -H "Content-Type: application/json"

# Script de synchronisation
cat > /scripts/qdrant-sync.sh << 'EOF'
#!/bin/bash
SNAPSHOT_NAME=$(curl -s "http://qdrant-master:6333/collections/enterprise_rag/snapshots" | jq -r '.result[-1].name')
curl -X GET "http://qdrant-master:6333/collections/enterprise_rag/snapshots/$SNAPSHOT_NAME" \
  --output "/backups/qdrant-${SNAPSHOT_NAME}.snapshot"

# Upload vers le site DR
aws s3 cp "/backups/qdrant-${SNAPSHOT_NAME}.snapshot" \
  "s3://rag-dr-backups/qdrant/" --region eu-central-1
EOF

# Cron job pour synchronisation (toutes les 6 heures)
echo "0 */6 * * * /scripts/qdrant-sync.sh" | crontab -
```

## Proc√©dures de Sauvegarde

### Sauvegarde Compl√®te Automatis√©e

#### Script Principal de Sauvegarde
```bash
#!/bin/bash
# /scripts/dr-backup.sh

set -euo pipefail

BACKUP_DATE=$(date +%Y%m%d_%H%M%S)
BACKUP_DIR="/backups/full-${BACKUP_DATE}"
S3_BUCKET="s3://rag-dr-backups"
RETENTION_DAYS=30

log_info() {
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] INFO: $1"
}

log_error() {
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] ERROR: $1" >&2
}

create_backup_directory() {
    log_info "Cr√©ation du r√©pertoire de sauvegarde: $BACKUP_DIR"
    mkdir -p "$BACKUP_DIR"
}

backup_database() {
    log_info "Sauvegarde de la base de donn√©es PostgreSQL"
    
    kubectl exec deployment/postgres -n enterprise-rag -- \
        pg_dump -U rag_user -h localhost rag_database \
        --no-password --clean --if-exists \
        > "${BACKUP_DIR}/database.sql"
    
    # Compression et chiffrement
    gzip "${BACKUP_DIR}/database.sql"
    gpg --symmetric --cipher-algo AES256 \
        --passphrase-file /secrets/backup-passphrase.txt \
        "${BACKUP_DIR}/database.sql.gz"
    
    rm "${BACKUP_DIR}/database.sql.gz"
    log_info "Sauvegarde base de donn√©es termin√©e"
}

backup_redis() {
    log_info "Sauvegarde du cache Redis"
    
    # D√©clenchement du BGSAVE
    kubectl exec deployment/redis -n enterprise-rag -- redis-cli BGSAVE
    
    # Attente de la fin du BGSAVE
    while kubectl exec deployment/redis -n enterprise-rag -- redis-cli LASTSAVE | \
          grep -q "$(kubectl exec deployment/redis -n enterprise-rag -- redis-cli LASTSAVE)"; do
        sleep 5
    done
    
    # Copie du dump RDB
    kubectl cp enterprise-rag/redis-pod:/data/dump.rdb "${BACKUP_DIR}/redis-dump.rdb"
    
    # Chiffrement
    gpg --symmetric --cipher-algo AES256 \
        --passphrase-file /secrets/backup-passphrase.txt \
        "${BACKUP_DIR}/redis-dump.rdb"
    
    rm "${BACKUP_DIR}/redis-dump.rdb"
    log_info "Sauvegarde Redis termin√©e"
}

backup_qdrant() {
    log_info "Sauvegarde de Qdrant"
    
    # Cr√©ation du snapshot
    SNAPSHOT_RESULT=$(curl -s -X POST \
        "http://qdrant.enterprise-rag.svc.cluster.local:6333/collections/enterprise_rag/snapshots")
    
    SNAPSHOT_NAME=$(echo "$SNAPSHOT_RESULT" | jq -r '.result.name')
    
    # T√©l√©chargement du snapshot
    curl -X GET \
        "http://qdrant.enterprise-rag.svc.cluster.local:6333/collections/enterprise_rag/snapshots/$SNAPSHOT_NAME" \
        --output "${BACKUP_DIR}/qdrant-snapshot.bin"
    
    # Chiffrement
    gpg --symmetric --cipher-algo AES256 \
        --passphrase-file /secrets/backup-passphrase.txt \
        "${BACKUP_DIR}/qdrant-snapshot.bin"
    
    rm "${BACKUP_DIR}/qdrant-snapshot.bin"
    log_info "Sauvegarde Qdrant termin√©e"
}

backup_minio() {
    log_info "Sauvegarde de MinIO"
    
    # Synchronisation avec AWS S3
    mc mirror --overwrite minio-local/rag-documents "${BACKUP_DIR}/minio-data/"
    
    # Archive et chiffrement
    tar -czf "${BACKUP_DIR}/minio-data.tar.gz" -C "${BACKUP_DIR}" minio-data/
    rm -rf "${BACKUP_DIR}/minio-data/"
    
    gpg --symmetric --cipher-algo AES256 \
        --passphrase-file /secrets/backup-passphrase.txt \
        "${BACKUP_DIR}/minio-data.tar.gz"
    
    rm "${BACKUP_DIR}/minio-data.tar.gz"
    log_info "Sauvegarde MinIO termin√©e"
}

backup_configurations() {
    log_info "Sauvegarde des configurations"
    
    # ConfigMaps
    kubectl get configmaps -n enterprise-rag -o yaml > "${BACKUP_DIR}/configmaps.yaml"
    
    # Secrets (sans les donn√©es sensibles)
    kubectl get secrets -n enterprise-rag -o yaml | \
        sed 's/data:/data: #REDACTED/g' > "${BACKUP_DIR}/secrets-template.yaml"
    
    # Deployments
    kubectl get deployments -n enterprise-rag -o yaml > "${BACKUP_DIR}/deployments.yaml"
    
    # Services
    kubectl get services -n enterprise-rag -o yaml > "${BACKUP_DIR}/services.yaml"
    
    # Ingress
    kubectl get ingress -n enterprise-rag -o yaml > "${BACKUP_DIR}/ingress.yaml"
    
    # Helm values
    helm get values rag-system -n enterprise-rag > "${BACKUP_DIR}/helm-values.yaml"
    
    # Archive
    tar -czf "${BACKUP_DIR}/configurations.tar.gz" -C "${BACKUP_DIR}" *.yaml
    rm "${BACKUP_DIR}"/*.yaml
    
    log_info "Sauvegarde configurations termin√©e"
}

upload_to_s3() {
    log_info "Upload vers S3"
    
    # Upload vers S3 avec m√©tadonn√©es
    aws s3 sync "$BACKUP_DIR" "$S3_BUCKET/full-backups/${BACKUP_DATE}/" \
        --metadata "backup-date=${BACKUP_DATE},backup-type=full,environment=production"
    
    # V√©rification de l'int√©grit√©
    aws s3 ls "$S3_BUCKET/full-backups/${BACKUP_DATE}/" --recursive | \
        awk '{print $4}' > "${BACKUP_DIR}/s3-inventory.txt"
    
    aws s3 cp "${BACKUP_DIR}/s3-inventory.txt" "$S3_BUCKET/full-backups/${BACKUP_DATE}/"
    
    log_info "Upload S3 termin√©"
}

cleanup_old_backups() {
    log_info "Nettoyage des anciennes sauvegardes"
    
    # Nettoyage local
    find /backups -name "full-*" -type d -mtime +7 -exec rm -rf {} +
    
    # Nettoyage S3 (garder les sauvegardes > 30 jours)
    aws s3 ls "$S3_BUCKET/full-backups/" | \
        awk '$1 < "'$(date -d "${RETENTION_DAYS} days ago" +%Y-%m-%d)'" {print $2}' | \
        while read dir; do
            aws s3 rm "$S3_BUCKET/full-backups/$dir" --recursive
        done
    
    log_info "Nettoyage termin√©"
}

verify_backup() {
    log_info "V√©rification de l'int√©grit√© de la sauvegarde"
    
    # V√©rification de la pr√©sence des fichiers critiques
    local required_files=(
        "database.sql.gpg"
        "redis-dump.rdb.gpg"
        "qdrant-snapshot.bin.gpg"
        "minio-data.tar.gz.gpg"
        "configurations.tar.gz"
    )
    
    for file in "${required_files[@]}"; do
        if [[ ! -f "${BACKUP_DIR}/${file}" ]]; then
            log_error "Fichier manquant: $file"
            exit 1
        fi
    done
    
    # Test de d√©chiffrement (sans extraction compl√®te)
    echo "test" | gpg --passphrase-file /secrets/backup-passphrase.txt \
        --decrypt "${BACKUP_DIR}/database.sql.gpg" | head -n 1 > /dev/null
    
    if [[ $? -ne 0 ]]; then
        log_error "Erreur de d√©chiffrement de la sauvegarde"
        exit 1
    fi
    
    log_info "V√©rification de l'int√©grit√© r√©ussie"
}

notify_completion() {
    log_info "Envoi des notifications"
    
    # Slack notification
    if [[ -n "${SLACK_WEBHOOK_URL:-}" ]]; then
        curl -X POST "$SLACK_WEBHOOK_URL" \
            -H 'Content-type: application/json' \
            --data "{
                \"text\": \"‚úÖ Sauvegarde DR compl√©t√©e\",
                \"attachments\": [{
                    \"color\": \"good\",
                    \"fields\": [
                        {\"title\": \"Date\", \"value\": \"${BACKUP_DATE}\", \"short\": true},
                        {\"title\": \"Taille\", \"value\": \"$(du -sh $BACKUP_DIR | cut -f1)\", \"short\": true},
                        {\"title\": \"Localisation\", \"value\": \"$S3_BUCKET/full-backups/${BACKUP_DATE}/\", \"short\": false}
                    ]
                }]
            }"
    fi
    
    # Email pour l'√©quipe
    echo "Sauvegarde DR compl√©t√©e avec succ√®s
    
Date: ${BACKUP_DATE}
Localisation: $S3_BUCKET/full-backups/${BACKUP_DATE}/
Taille: $(du -sh $BACKUP_DIR | cut -f1)
    
Fichiers sauvegard√©s:
$(ls -la $BACKUP_DIR)
" | mail -s "Sauvegarde DR - $BACKUP_DATE" ops-team@company.com
    
    log_info "Notifications envoy√©es"
}

main() {
    log_info "D√©but de la sauvegarde DR compl√®te"
    
    create_backup_directory
    backup_database
    backup_redis
    backup_qdrant
    backup_minio
    backup_configurations
    verify_backup
    upload_to_s3
    cleanup_old_backups
    notify_completion
    
    log_info "Sauvegarde DR compl√®te termin√©e avec succ√®s"
}

# Ex√©cution avec gestion d'erreur
if ! main; then
    log_error "√âchec de la sauvegarde DR"
    # Notification d'√©chec
    curl -X POST "$SLACK_WEBHOOK_URL" \
        -H 'Content-type: application/json' \
        --data '{"text": "‚ùå √âchec de la sauvegarde DR", "channel": "#alerts"}'
    exit 1
fi
```

#### Planification des Sauvegardes
```bash
# Crontab pour les sauvegardes automatis√©es
# /etc/crontab

# Sauvegarde compl√®te quotidienne √† 2h00
0 2 * * * root /scripts/dr-backup.sh

# Sauvegarde incr√©mentale toutes les 6 heures
0 */6 * * * root /scripts/incremental-backup.sh

# Sauvegarde des WAL PostgreSQL toutes les 15 minutes
*/15 * * * * postgres /scripts/wal-backup.sh

# V√©rification de l'int√©grit√© hebdomadaire
0 3 * * 0 root /scripts/verify-backups.sh
```

## Proc√©dures de Restauration

### Restauration Compl√®te du Syst√®me

#### Script de Restauration
```bash
#!/bin/bash
# /scripts/dr-restore.sh

set -euo pipefail

BACKUP_DATE=${1:-latest}
RESTORE_ENVIRONMENT=${2:-production}
S3_BUCKET="s3://rag-dr-backups"
RESTORE_DIR="/restore/${BACKUP_DATE}"

log_info() {
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] INFO: $1"
}

prepare_restore_environment() {
    log_info "Pr√©paration de l'environnement de restauration"
    
    # Cr√©ation du namespace si n√©cessaire
    kubectl create namespace "enterprise-rag-${RESTORE_ENVIRONMENT}" --dry-run=client -o yaml | kubectl apply -f -
    
    # Arr√™t des services existants si restauration en place
    if [[ "$RESTORE_ENVIRONMENT" == "production" ]]; then
        kubectl scale deployment --all --replicas=0 -n enterprise-rag
        sleep 30
    fi
    
    # Pr√©paration du r√©pertoire de restauration
    mkdir -p "$RESTORE_DIR"
    cd "$RESTORE_DIR"
}

download_backup() {
    log_info "T√©l√©chargement de la sauvegarde depuis S3"
    
    if [[ "$BACKUP_DATE" == "latest" ]]; then
        BACKUP_DATE=$(aws s3 ls "$S3_BUCKET/full-backups/" | sort | tail -n 1 | awk '{print $2}' | tr -d '/')
    fi
    
    aws s3 sync "$S3_BUCKET/full-backups/${BACKUP_DATE}/" "$RESTORE_DIR/"
    
    log_info "Sauvegarde t√©l√©charg√©e: $BACKUP_DATE"
}

restore_database() {
    log_info "Restauration de la base de donn√©es"
    
    # D√©chiffrement
    gpg --batch --yes --passphrase-file /secrets/backup-passphrase.txt \
        --decrypt "${RESTORE_DIR}/database.sql.gpg" | gunzip > "${RESTORE_DIR}/database.sql"
    
    # Attente que PostgreSQL soit pr√™t
    kubectl wait --for=condition=ready pod -l app=postgres -n "enterprise-rag-${RESTORE_ENVIRONMENT}" --timeout=300s
    
    # Restauration
    kubectl exec -i deployment/postgres -n "enterprise-rag-${RESTORE_ENVIRONMENT}" -- \
        psql -U rag_user -d rag_database < "${RESTORE_DIR}/database.sql"
    
    log_info "Base de donn√©es restaur√©e"
}

restore_redis() {
    log_info "Restauration de Redis"
    
    # D√©chiffrement
    gpg --batch --yes --passphrase-file /secrets/backup-passphrase.txt \
        --decrypt "${RESTORE_DIR}/redis-dump.rdb.gpg" > "${RESTORE_DIR}/dump.rdb"
    
    # Arr√™t de Redis
    kubectl scale deployment redis --replicas=0 -n "enterprise-rag-${RESTORE_ENVIRONMENT}"
    sleep 10
    
    # Copie du fichier de dump
    kubectl cp "${RESTORE_DIR}/dump.rdb" "enterprise-rag-${RESTORE_ENVIRONMENT}/redis-pod:/data/dump.rdb"
    
    # Red√©marrage de Redis
    kubectl scale deployment redis --replicas=1 -n "enterprise-rag-${RESTORE_ENVIRONMENT}"
    kubectl wait --for=condition=ready pod -l app=redis -n "enterprise-rag-${RESTORE_ENVIRONMENT}" --timeout=300s
    
    log_info "Redis restaur√©"
}

restore_qdrant() {
    log_info "Restauration de Qdrant"
    
    # D√©chiffrement
    gpg --batch --yes --passphrase-file /secrets/backup-passphrase.txt \
        --decrypt "${RESTORE_DIR}/qdrant-snapshot.bin.gpg" > "${RESTORE_DIR}/qdrant-snapshot.bin"
    
    # Attente que Qdrant soit pr√™t
    kubectl wait --for=condition=ready pod -l app=qdrant -n "enterprise-rag-${RESTORE_ENVIRONMENT}" --timeout=300s
    
    # Suppression de la collection existante
    curl -X DELETE "http://qdrant.enterprise-rag-${RESTORE_ENVIRONMENT}.svc.cluster.local:6333/collections/enterprise_rag"
    
    # Restauration du snapshot
    curl -X PUT "http://qdrant.enterprise-rag-${RESTORE_ENVIRONMENT}.svc.cluster.local:6333/collections/enterprise_rag/snapshots/upload" \
        --data-binary "@${RESTORE_DIR}/qdrant-snapshot.bin"
    
    log_info "Qdrant restaur√©"
}

restore_minio() {
    log_info "Restauration de MinIO"
    
    # D√©chiffrement et extraction
    gpg --batch --yes --passphrase-file /secrets/backup-passphrase.txt \
        --decrypt "${RESTORE_DIR}/minio-data.tar.gz.gpg" | tar -xzf - -C "$RESTORE_DIR"
    
    # Attente que MinIO soit pr√™t
    kubectl wait --for=condition=ready pod -l app=minio -n "enterprise-rag-${RESTORE_ENVIRONMENT}" --timeout=300s
    
    # Restoration via mc
    mc alias set minio-restore "http://minio.enterprise-rag-${RESTORE_ENVIRONMENT}.svc.cluster.local:9000" \
        "$(kubectl get secret minio-secrets -n enterprise-rag-${RESTORE_ENVIRONMENT} -o jsonpath='{.data.access-key}' | base64 -d)" \
        "$(kubectl get secret minio-secrets -n enterprise-rag-${RESTORE_ENVIRONMENT} -o jsonpath='{.data.secret-key}' | base64 -d)"
    
    mc mirror "${RESTORE_DIR}/minio-data/" minio-restore/rag-documents/ --overwrite
    
    log_info "MinIO restaur√©"
}

restore_configurations() {
    log_info "Restauration des configurations"
    
    # Extraction des configurations
    tar -xzf "${RESTORE_DIR}/configurations.tar.gz" -C "$RESTORE_DIR"
    
    # Application des configurations (en excluant les secrets pour la s√©curit√©)
    kubectl apply -f "${RESTORE_DIR}/configmaps.yaml" -n "enterprise-rag-${RESTORE_ENVIRONMENT}"
    kubectl apply -f "${RESTORE_DIR}/deployments.yaml" -n "enterprise-rag-${RESTORE_ENVIRONMENT}"
    kubectl apply -f "${RESTORE_DIR}/services.yaml" -n "enterprise-rag-${RESTORE_ENVIRONMENT}"
    kubectl apply -f "${RESTORE_DIR}/ingress.yaml" -n "enterprise-rag-${RESTORE_ENVIRONMENT}"
    
    log_info "Configurations restaur√©es"
}

restart_services() {
    log_info "Red√©marrage des services"
    
    # Red√©marrage s√©quentiel des services
    local services=("postgres" "redis" "qdrant" "minio" "rag-api" "rag-celery")
    
    for service in "${services[@]}"; do
        log_info "Red√©marrage de $service"
        kubectl rollout restart "deployment/$service" -n "enterprise-rag-${RESTORE_ENVIRONMENT}"
        kubectl rollout status "deployment/$service" -n "enterprise-rag-${RESTORE_ENVIRONMENT}" --timeout=300s
    done
    
    log_info "Tous les services red√©marr√©s"
}

verify_restoration() {
    log_info "V√©rification de la restauration"
    
    # Tests de sant√©
    local api_url
    if [[ "$RESTORE_ENVIRONMENT" == "production" ]]; then
        api_url="https://api.rag-system.com"
    else
        api_url="https://${RESTORE_ENVIRONMENT}-api.rag-system.com"
    fi
    
    # Attendre que l'API soit disponible
    local retries=0
    while [[ $retries -lt 30 ]]; do
        if curl -f "$api_url/health" &> /dev/null; then
            log_info "API accessible"
            break
        fi
        retries=$((retries + 1))
        sleep 10
    done
    
    if [[ $retries -eq 30 ]]; then
        log_error "API non accessible apr√®s restauration"
        return 1
    fi
    
    # Tests fonctionnels
    kubectl run restore-test --rm -i --restart=Never \
        --image=curlimages/curl:latest \
        --namespace="enterprise-rag-${RESTORE_ENVIRONMENT}" \
        -- sh -c "
            curl -X GET '$api_url/admin/health/database' -H 'Authorization: Bearer \$ADMIN_TOKEN' | grep -q 'healthy' &&
            curl -X GET '$api_url/admin/health/redis' -H 'Authorization: Bearer \$ADMIN_TOKEN' | grep -q 'healthy' &&
            curl -X GET '$api_url/admin/health/qdrant' -H 'Authorization: Bearer \$ADMIN_TOKEN' | grep -q 'healthy'
        "
    
    if [[ $? -eq 0 ]]; then
        log_info "V√©rification de la restauration r√©ussie"
    else
        log_error "√âchec de la v√©rification de la restauration"
        return 1
    fi
}

cleanup_restore() {
    log_info "Nettoyage des fichiers de restauration"
    rm -rf "$RESTORE_DIR"
}

main() {
    log_info "D√©but de la restauration DR"
    
    prepare_restore_environment
    download_backup
    restore_database
    restore_redis
    restore_qdrant
    restore_minio
    restore_configurations
    restart_services
    verify_restoration
    cleanup_restore
    
    log_info "Restauration DR termin√©e avec succ√®s"
}

# Ex√©cution
main
```

## Proc√©dures de Basculement

### Basculement Automatique (Failover)

#### Configuration DNS avec Health Check
```bash
# Configuration Route53 pour basculement automatique
aws route53 change-resource-record-sets --hosted-zone-id Z123456789 --change-batch '{
  "Changes": [{
    "Action": "UPSERT",
    "ResourceRecordSet": {
      "Name": "api.rag-system.com",
      "Type": "A",
      "SetIdentifier": "primary",
      "Failover": "PRIMARY",
      "TTL": 60,
      "ResourceRecords": [{"Value": "1.2.3.4"}],
      "HealthCheckId": "health-check-primary"
    }
  }, {
    "Action": "UPSERT",
    "ResourceRecordSet": {
      "Name": "api.rag-system.com", 
      "Type": "A",
      "SetIdentifier": "secondary",
      "Failover": "SECONDARY",
      "TTL": 60,
      "ResourceRecords": [{"Value": "5.6.7.8"}]
    }
  }]
}'

# Health check pour le site principal
aws route53 create-health-check --caller-reference "$(date +%s)" --health-check-config '{
  "Type": "HTTPS",
  "ResourcePath": "/health",
  "FullyQualifiedDomainName": "api.rag-system.com",
  "Port": 443,
  "RequestInterval": 30,
  "FailureThreshold": 3
}'
```

#### Script de Basculement Manuel
```bash
#!/bin/bash
# /scripts/manual-failover.sh

set -euo pipefail

TARGET_SITE=${1:-dr}  # primary, dr, cold
REASON=${2:-"Manual failover initiated"}

log_info() {
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] INFO: $1"
}

validate_target_site() {
    log_info "Validation du site cible: $TARGET_SITE"
    
    case $TARGET_SITE in
        "primary")
            KUBE_CONTEXT="production-eu-west-1"
            DNS_RECORD="1.2.3.4"
            ;;
        "dr")
            KUBE_CONTEXT="dr-eu-central-1"
            DNS_RECORD="5.6.7.8"
            ;;
        "cold")
            log_info "D√©ploiement du site cold en cours..."
            deploy_cold_site
            KUBE_CONTEXT="cold-us-east-1"
            DNS_RECORD="9.10.11.12"
            ;;
        *)
            echo "Site invalide: $TARGET_SITE"
            exit 1
            ;;
    esac
}

check_site_health() {
    log_info "V√©rification de la sant√© du site cible"
    
    kubectl --context="$KUBE_CONTEXT" get pods -n enterprise-rag | grep -q "Running"
    
    if [[ $? -ne 0 ]]; then
        log_error "Site cible non sain, annulation du basculement"
        exit 1
    fi
}

update_dns() {
    log_info "Mise √† jour des enregistrements DNS"
    
    # Mise √† jour du record principal
    aws route53 change-resource-record-sets --hosted-zone-id Z123456789 --change-batch "{
      \"Changes\": [{
        \"Action\": \"UPSERT\",
        \"ResourceRecordSet\": {
          \"Name\": \"api.rag-system.com\",
          \"Type\": \"A\",
          \"TTL\": 60,
          \"ResourceRecords\": [{\"Value\": \"$DNS_RECORD\"}]
        }
      }]
    }"
    
    # Attendre la propagation DNS
    sleep 120
}

sync_data_if_needed() {
    if [[ "$TARGET_SITE" != "primary" ]]; then
        log_info "Synchronisation des donn√©es vers le site cible"
        
        # Synchronisation de la base de donn√©es si n√©cessaire
        /scripts/sync-database.sh --target="$TARGET_SITE"
        
        # Synchronisation des fichiers
        /scripts/sync-files.sh --target="$TARGET_SITE"
    fi
}

update_monitoring() {
    log_info "Mise √† jour de la configuration de monitoring"
    
    # Mise √† jour des targets Prometheus
    kubectl --context="$KUBE_CONTEXT" patch configmap prometheus-config -n enterprise-rag --patch "
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
    scrape_configs:
      - job_name: 'rag-api'
        static_configs:
          - targets: ['rag-api.enterprise-rag.svc.cluster.local:8000']
    "
    
    # Red√©marrage de Prometheus
    kubectl --context="$KUBE_CONTEXT" rollout restart deployment/prometheus -n enterprise-rag
}

notify_teams() {
    log_info "Notification des √©quipes"
    
    # Slack
    curl -X POST "$SLACK_WEBHOOK_URL" \
        -H 'Content-type: application/json' \
        --data "{
            \"text\": \"üîÑ Basculement vers $TARGET_SITE\",
            \"attachments\": [{
                \"color\": \"warning\",
                \"fields\": [
                    {\"title\": \"Site cible\", \"value\": \"$TARGET_SITE\", \"short\": true},
                    {\"title\": \"Raison\", \"value\": \"$REASON\", \"short\": true},
                    {\"title\": \"Timestamp\", \"value\": \"$(date -u)\", \"short\": true}
                ]
            }]
        }"
    
    # Email d'urgence
    echo "Basculement d'urgence effectu√© vers $TARGET_SITE
    
Raison: $REASON
Timestamp: $(date -u)
Site actif: $TARGET_SITE
DNS pointant vers: $DNS_RECORD

Actions suivantes requises:
1. V√©rifier le fonctionnement du service
2. Investiguer la cause du basculement
3. Planifier le retour au site principal

Contacts d'escalade:
- DevOps: +33 X XX XX XX XX
- Management: +33 X XX XX XX XX
" | mail -s "URGENT: Basculement DR vers $TARGET_SITE" ops-team@company.com
}

deploy_cold_site() {
    log_info "D√©ploiement du site cold"
    
    # Terraform pour l'infrastructure
    cd /terraform/cold-site
    terraform init
    terraform apply -auto-approve
    
    # D√©ploiement Kubernetes
    kubectl config use-context cold-us-east-1
    helm install rag-system /charts/rag-system \
        --namespace enterprise-rag \
        --create-namespace \
        --values /config/cold-site-values.yaml \
        --wait --timeout=600s
    
    # Restauration des donn√©es
    /scripts/dr-restore.sh latest cold
}

main() {
    log_info "D√©but du basculement vers $TARGET_SITE"
    log_info "Raison: $REASON"
    
    validate_target_site
    check_site_health
    sync_data_if_needed
    update_dns
    update_monitoring
    notify_teams
    
    log_info "Basculement vers $TARGET_SITE termin√© avec succ√®s"
}

# Confirmation avant ex√©cution
echo "ATTENTION: Vous allez effectuer un basculement vers $TARGET_SITE"
echo "Raison: $REASON"
echo "Confirmer? (yes/no)"
read -r confirmation

if [[ "$confirmation" == "yes" ]]; then
    main
else
    echo "Basculement annul√©"
    exit 0
fi
```

## Tests de Reprise d'Activit√©

### Plan de Tests R√©guliers

#### Tests Mensuels
```bash
#!/bin/bash
# /scripts/dr-test-monthly.sh

# Test de basculement sur site DR
log_info "Test de basculement mensuel"

# 1. Notification de d√©but de test
notify_test_start() {
    curl -X POST "$SLACK_WEBHOOK_URL" \
        -H 'Content-type: application/json' \
        --data '{"text": "üß™ D√©but du test DR mensuel", "channel": "#ops"}'
}

# 2. Basculement vers DR
failover_to_dr() {
    ./manual-failover.sh dr "Test mensuel programm√©"
}

# 3. Tests fonctionnels
run_functional_tests() {
    # Test de l'API
    curl -f "https://api.rag-system.com/health"
    
    # Test de l'authentification
    curl -X POST "https://api.rag-system.com/auth/login" \
        -d '{"username":"test","password":"test"}'
    
    # Test de recherche
    curl -X POST "https://api.rag-system.com/search" \
        -H "Authorization: Bearer $TEST_TOKEN" \
        -d '{"query":"test query"}'
}

# 4. Retour au site principal
failback_to_primary() {
    sleep 3600  # 1 heure de test
    ./manual-failover.sh primary "Fin du test DR mensuel"
}

# 5. Rapport de test
generate_test_report() {
    echo "Test DR mensuel - $(date)
    
R√©sultats:
- Basculement vers DR: ‚úÖ
- Tests fonctionnels: ‚úÖ  
- Retour au principal: ‚úÖ
- Dur√©e totale: 1h15min
- RTO observ√©: 5 minutes
- RPO observ√©: < 1 minute

Commentaires:
- Tous les services ont bascul√© correctement
- Aucune perte de donn√©es d√©tect√©e
- Performance acceptable sur le site DR
" > "/reports/dr-test-$(date +%Y%m%d).txt"
    
    # Envoi du rapport
    mail -s "Rapport test DR mensuel" ops-team@company.com < "/reports/dr-test-$(date +%Y%m%d).txt"
}

# Ex√©cution s√©quentielle
notify_test_start
failover_to_dr
run_functional_tests
failback_to_primary
generate_test_report
```

#### Tests Trimestriels Complets
```bash
#!/bin/bash
# /scripts/dr-test-quarterly.sh

# Test complet incluant restauration depuis backup

# 1. Cr√©ation d'un environnement de test isol√©
create_test_environment() {
    kubectl create namespace dr-test-$(date +%Y%m%d)
    
    # D√©ploiement de l'infrastructure de test
    helm install rag-test /charts/rag-system \
        --namespace dr-test-$(date +%Y%m%d) \
        --values /config/test-values.yaml
}

# 2. Test de restauration compl√®te
test_full_restore() {
    # Utilisation d'une sauvegarde de la semaine derni√®re
    BACKUP_DATE=$(date -d '7 days ago' +%Y%m%d)
    
    /scripts/dr-restore.sh "$BACKUP_DATE" "dr-test-$(date +%Y%m%d)"
}

# 3. Tests de performance
performance_tests() {
    # Tests de charge avec Locust
    cd /tests/load
    locust -f locustfile.py --host="https://test-api.rag-system.com" \
        --users 100 --spawn-rate 10 --run-time 30m --headless
}

# 4. Tests de continuit√© m√©tier
business_continuity_tests() {
    # Simulation de charge utilisateur normale
    # V√©rification des SLAs
    # Tests de tous les workflows critiques
    
    python /tests/business-continuity/run_all_tests.py \
        --environment "dr-test-$(date +%Y%m%d)"
}

# 5. Nettoyage
cleanup_test_environment() {
    kubectl delete namespace "dr-test-$(date +%Y%m%d)"
}

# Ex√©cution du test complet
main() {
    create_test_environment
    test_full_restore
    performance_tests
    business_continuity_tests
    cleanup_test_environment
    
    # G√©n√©ration du rapport complet
    generate_quarterly_report
}

main
```

## Communication et Escalade

### Matrice de Communication

#### Niveaux d'Incident

**Niveau 1 - Critique**
- **D√©finition** : Service compl√®tement indisponible
- **RTO** : 15 minutes
- **Communication** : Imm√©diate
- **Escalade** : CTO + VP Engineering

**Niveau 2 - Majeur** 
- **D√©finition** : D√©gradation significative
- **RTO** : 2 heures
- **Communication** : Dans les 30 minutes
- **Escalade** : Engineering Manager

**Niveau 3 - Mineur**
- **D√©finition** : Impact limit√©
- **RTO** : 8 heures
- **Communication** : Dans les 2 heures
- **Escalade** : Lead DevOps

#### Canaux de Communication
- **Slack** : #incidents (temps r√©el)
- **Email** : ops-team@company.com
- **T√©l√©phone** : +33 X XX XX XX XX (astreinte)
- **StatusPage** : https://status.rag-system.com

### Proc√©dures d'Escalade

#### Timeline d'Escalade
```
T+0   : D√©tection de l'incident
T+5   : Notification √©quipe DevOps
T+15  : Escalade N1 si non r√©solu
T+30  : Notification management
T+60  : Escalade N2 + communication externe
T+120 : Escalade executive + plan de communication publique
```

#### Contacts d'Urgence
- **DevOps Lead** : +33 1 XX XX XX XX
- **Engineering Manager** : +33 1 XX XX XX XX  
- **CTO** : +33 1 XX XX XX XX
- **VP Engineering** : +33 1 XX XX XX XX

## M√©triques et KPIs DR

### Indicateurs de Performance

#### Temps de R√©cup√©ration (RTO)
- **Cible** : < 2 heures
- **Mesure** : Temps entre d√©tection et service restaur√©
- **Seuil d'alerte** : > 4 heures

#### Point de R√©cup√©ration (RPO)
- **Cible** : < 1 heure
- **Mesure** : Perte de donn√©es maximale
- **Seuil d'alerte** : > 2 heures

#### Disponibilit√©
- **Cible** : 99.9% (8.76 heures de downtime/an)
- **Mesure** : Uptime total sur p√©riode
- **Seuil d'alerte** : < 99.5%

### Tableaux de Bord DR

#### Grafana Dashboard "Disaster Recovery"
```yaml
# Configuration du dashboard DR
dashboard:
  title: "Disaster Recovery Monitoring"
  panels:
    - title: "Backup Success Rate"
      type: "stat"
      query: "increase(backup_success_total[24h]) / increase(backup_attempts_total[24h]) * 100"
      
    - title: "Recovery Time Objective"
      type: "gauge"
      query: "histogram_quantile(0.95, rate(recovery_time_seconds_bucket[24h]))"
      
    - title: "Cross-Site Replication Lag"
      type: "graph"
      query: "replication_lag_seconds"
      
    - title: "DR Site Health"
      type: "table"
      query: "up{job=~'.*-dr'}"
```

## Formation et Documentation

### Formation de l'√âquipe

#### Programme de Formation DR
1. **Formation initiale** (8h)
   - Principes du DR
   - Architecture du syst√®me
   - Proc√©dures de base

2. **Formation avanc√©e** (16h)
   - Troubleshooting complexe
   - Optimisation des performances
   - Gestion de crise

3. **Exercices pratiques** (mensuel)
   - Simulations d'incidents
   - Tests de proc√©dures
   - Am√©lioration continue

#### Certification DR
- **Niveau 1** : Op√©rateur DR
- **Niveau 2** : Sp√©cialiste DR  
- **Niveau 3** : Expert DR

### Documentation Associ√©e

#### Documents de R√©f√©rence
- [Architecture de Haute Disponibilit√©](./ha-architecture.md)
- [Runbook Incidents](./runbooks/incident-response.md)
- [Guide de Monitoring](./monitoring-guide.md)
- [Proc√©dures de S√©curit√©](./security-procedures.md)

---

*Ce plan de reprise d'activit√© est un document vivant qui doit √™tre r√©guli√®rement mis √† jour et test√©. Derni√®re r√©vision : D√©cembre 2024*
