#!/usr/bin/env python3
"""
Service RAG avec Ollama - Version fonctionnelle
"""
import uvicorn
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import ollama
import httpx
from datetime import datetime
from typing import List, Optional
import asyncio

app = FastAPI(
    title="MAR RAG System with Ollama",
    description="Syst√®me RAG Multi-Agents avec Ollama LLM",
    version="2.0.0"
)

class QueryRequest(BaseModel):
    question: str
    model: Optional[str] = "llama3.2:3b"
    use_context: Optional[bool] = True

class QueryResponse(BaseModel):
    answer: str
    model_used: str
    processing_time: float
    status: str = "success"
    context_used: Optional[List[str]] = None

class RAGService:
    def __init__(self):
        self.client = ollama.Client()
        self.knowledge_base = [
            "Le syst√®me MAR (Multi-Agent RAG) utilise plusieurs agents sp√©cialis√©s pour traiter les requ√™tes.",
            "Les agents incluent : Agent de Recherche, Agent d'Analyse, Agent de Synth√®se, et Agent de Validation.",
            "Le syst√®me utilise Qdrant pour la recherche vectorielle et PostgreSQL pour le stockage des donn√©es.",
            "FastAPI est utilis√© comme framework web principal avec uvicorn comme serveur ASGI.",
            "L'architecture microservices permet une scalabilit√© horizontale et une maintenance facilit√©e.",
            "Les embeddings sont g√©n√©r√©s via des mod√®les Sentence Transformers pour la recherche s√©mantique.",
            "Redis est utilis√© pour le cache et la gestion des sessions utilisateur.",
            "Le monitoring se fait via Prometheus et Grafana pour les m√©triques en temps r√©el."
        ]
    
    async def check_ollama_status(self) -> bool:
        """V√©rifier si Ollama est disponible"""
        try:
            async with httpx.AsyncClient() as client:
                response = await client.get("http://localhost:11434/api/version")
                return response.status_code == 200
        except:
            return False
    
    def find_relevant_context(self, question: str) -> List[str]:
        """Simuler la recherche de contexte pertinent"""
        question_lower = question.lower()
        relevant = []
        
        keywords = {
            "agent": ["agent", "multi-agent", "architecture"],
            "rag": ["rag", "recherche", "retrieval"],
            "database": ["base", "donn√©es", "postgresql", "qdrant"],
            "api": ["api", "fastapi", "endpoint"],
            "monitoring": ["monitoring", "m√©triques", "prometheus"],
            "cache": ["cache", "redis", "session"]
        }
        
        for category, terms in keywords.items():
            if any(term in question_lower for term in terms):
                relevant.extend([kb for kb in self.knowledge_base if any(term in kb.lower() for term in terms)])
        
        return list(set(relevant))[:3]  # Max 3 contextes pertinents
    
    async def generate_answer(self, question: str, context: List[str], model: str) -> str:
        """G√©n√©rer une r√©ponse avec Ollama"""
        context_text = "\n".join(context) if context else ""
        
        prompt = f"""Tu es un assistant expert en syst√®mes RAG et architecture multi-agents.

Contexte disponible:
{context_text}

Question: {question}

R√©ponds de mani√®re pr√©cise et technique en fran√ßais. Si le contexte ne contient pas d'information pertinente, r√©ponds quand m√™me avec tes connaissances g√©n√©rales sur les syst√®mes RAG."""

        try:
            response = await asyncio.to_thread(
                self.client.generate,
                model=model,
                prompt=prompt,
                stream=False
            )
            return response['response']
        except Exception as e:
            return f"Erreur lors de la g√©n√©ration: {str(e)}"

rag_service = RAGService()

@app.get("/")
async def root():
    ollama_status = await rag_service.check_ollama_status()
    return {
        "message": "üöÄ MAR RAG System with Ollama",
        "status": "running",
        "version": "2.0.0",
        "ollama_status": "connected" if ollama_status else "disconnected",
        "available_models": ["llama3.2:3b"],
        "endpoints": {
            "health": "/health",
            "query": "/api/v1/query",
            "models": "/api/v1/models",
            "docs": "/docs"
        }
    }

@app.get("/health")
async def health_check():
    ollama_status = await rag_service.check_ollama_status()
    return {
        "status": "healthy",
        "service": "MAR RAG API with Ollama",
        "timestamp": datetime.now().isoformat(),
        "ollama_connected": ollama_status
    }

@app.get("/api/v1/models")
async def list_models():
    """Lister les mod√®les disponibles"""
    try:
        models = await asyncio.to_thread(rag_service.client.list)
        return {
            "models": [model['name'] for model in models['models']],
            "status": "success"
        }
    except Exception as e:
        return {
            "models": [],
            "status": "error",
            "error": str(e)
        }

@app.post("/api/v1/query")
async def query(request: QueryRequest):
    """
    Endpoint principal pour les requ√™tes RAG avec Ollama
    """
    start_time = datetime.now()
    
    try:
        # V√©rifier la connexion Ollama
        if not await rag_service.check_ollama_status():
            raise HTTPException(status_code=503, detail="Ollama service non disponible")
        
        # Rechercher le contexte pertinent
        context = []
        if request.use_context:
            context = rag_service.find_relevant_context(request.question)
        
        # G√©n√©rer la r√©ponse
        answer = await rag_service.generate_answer(
            request.question, 
            context, 
            request.model
        )
        
        processing_time = (datetime.now() - start_time).total_seconds()
        
        return QueryResponse(
            answer=answer,
            model_used=request.model,
            processing_time=processing_time,
            context_used=context if request.use_context else None
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur: {str(e)}")

if __name__ == "__main__":
    print("üöÄ D√©marrage du syst√®me MAR RAG avec Ollama...")
    print("üì° API sera disponible sur: http://localhost:8001")
    print("üìö Documentation sur: http://localhost:8001/docs")
    print("ü§ñ Mod√®le par d√©faut: llama3.2:3b")
    
    uvicorn.run(
        "rag_service_ollama:app",
        host="0.0.0.0",
        port=8001,
        reload=True,
        log_level="info"
    )
