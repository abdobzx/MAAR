# Guide d'utilisation de la plateforme MAR

## Vue d'ensemble

La plateforme MAR (Multi-Agent RAG) est un systÃ¨me avancÃ© de traitement de documents utilisant plusieurs agents IA spÃ©cialisÃ©s pour fournir des rÃ©ponses contextuelles et prÃ©cises.

### Agents disponibles

1. **Retriever Agent** - Recherche vectorielle dans les documents
2. **Summarizer Agent** - RÃ©sumÃ© des documents pertinents  
3. **Synthesizer Agent** - SynthÃ¨se et gÃ©nÃ©ration de rÃ©ponses
4. **Critic Agent** - Validation et scoring de qualitÃ©
5. **Ranker Agent** - Classement et priorisation des rÃ©sultats

## FonctionnalitÃ©s principales

### Recherche intelligente
- Recherche vectorielle haute performance avec FAISS/Chroma
- Support de multiples formats : PDF, DOCX, TXT, MD, JSON
- Chunking intelligent avec overlap pour prÃ©server le contexte

### Traitement multi-agents
- Workflow orchestrÃ© par CrewAI
- Collaboration entre agents spÃ©cialisÃ©s
- Validation automatique de la qualitÃ© des rÃ©ponses

### ObservabilitÃ© complÃ¨te
- MÃ©triques Prometheus en temps rÃ©el
- Dashboards Grafana prÃ©configurÃ©
- Logs centralisÃ©s avec ELK Stack
- Tracing des workflows agents

## Architecture technique

### Composants principaux
- **API FastAPI** : Interface REST avec authentification JWT
- **Service Ollama** : LLM local (Llama3, Mistral, Phi3)
- **Vector Store** : FAISS ou ChromaDB pour la recherche
- **Interface Streamlit** : UI moderne et interactive
- **Stack monitoring** : Prometheus, Grafana, ELK

### DÃ©ploiement
- **Local** : Docker Compose pour dÃ©veloppement
- **Production** : Kubernetes avec haute disponibilitÃ©
- **CI/CD** : GitHub Actions avec tests automatisÃ©s
- **SÃ©curitÃ©** : JWT, rate limiting, CORS, NetworkPolicies

## Utilisation

### Installation rapide

```bash
# Cloner le projet
git clone https://github.com/your-org/mar-platform
cd mar-platform

# Installer les dÃ©pendances
make install

# Lancer la stack complÃ¨te
make up
```

### AccÃ¨s aux interfaces

- **Interface principale** : http://localhost:8501
- **API REST** : http://localhost:8000
- **Documentation API** : http://localhost:8000/docs
- **Grafana** : http://localhost:3000 (admin/mar_admin_2024)
- **Prometheus** : http://localhost:9090

### Utilisation de l'API

```python
import requests

# Poster une question
response = requests.post("http://localhost:8000/api/v1/chat", 
    json={
        "query": "Comment fonctionne la recherche vectorielle ?",
        "max_documents": 5,
        "include_validation": True
    }
)

print(response.json())
```

### Ingestion de documents

```bash
# Via l'interface Streamlit (recommandÃ©)
# Aller sur http://localhost:8501 > Documents > Upload

# Via l'API
curl -X POST "http://localhost:8000/api/v1/documents/ingest" \
  -H "Content-Type: multipart/form-data" \
  -F "file=@document.pdf"
```

## Configuration avancÃ©e

### Variables d'environnement

```bash
# Configuration LLM
OLLAMA_HOST=localhost:11434
OLLAMA_MODEL=llama3

# Configuration Vector Store  
VECTOR_STORE_TYPE=faiss  # ou chroma
VECTOR_STORE_PATH=/app/data/vector_store

# Configuration API
JWT_SECRET=your-secret-key
LOG_LEVEL=INFO
PROMETHEUS_ENABLED=true
```

### Personnalisation des agents

```python
# Dans orchestrator/crew/mar_crew.py
agent_config = {
    "retriever": {
        "max_documents": 10,
        "similarity_threshold": 0.7
    },
    "synthesizer": {
        "temperature": 0.5,
        "max_tokens": 2000
    }
}
```

## Monitoring et maintenance

### MÃ©triques clÃ©s Ã  surveiller

1. **Performance API**
   - Temps de rÃ©ponse moyen < 3s
   - Taux d'erreur < 1%
   - Throughput requests/sec

2. **Agents MAR**
   - Latence par agent
   - Score de qualitÃ© Critic > 85%
   - Taux de succÃ¨s retrieval > 95%

3. **Ressources systÃ¨me**
   - CPU < 80%
   - MÃ©moire < 80%  
   - Espace disque vector store

### Alertes automatiques

Les rÃ¨gles Prometheus incluent :
- API non accessible (30s)
- Temps de rÃ©ponse Ã©levÃ© (>5s pendant 2min)
- Taux d'erreur Ã©levÃ© (>5% pendant 2min)
- Utilisation mÃ©moire Ã©levÃ©e (>2GB pendant 5min)

### Sauvegarde et restauration

```bash
# Sauvegarde automatique
make backup

# Sauvegarde manuelle du vector store
docker exec mar-api tar -czf /app/backup-$(date +%Y%m%d).tar.gz /app/data/vector_store
```

## DÃ©veloppement

### Structure du projet

```
mar-platform/
â”œâ”€â”€ agents/          # Agents IA spÃ©cialisÃ©s
â”œâ”€â”€ api/            # API FastAPI
â”œâ”€â”€ orchestrator/   # Orchestrateur CrewAI  
â”œâ”€â”€ vector_store/   # Module vector store
â”œâ”€â”€ ui/             # Interface Streamlit
â”œâ”€â”€ llm/            # Client LLM Ollama
â”œâ”€â”€ k8s/            # Manifestes Kubernetes
â”œâ”€â”€ monitoring/     # Configuration monitoring
â””â”€â”€ tests/          # Tests automatisÃ©s
```

### Workflow de dÃ©veloppement

```bash
# Installer l'environnement de dev
make install-dev

# Lancer en mode dÃ©veloppement
make dev

# ExÃ©cuter les tests
make test

# VÃ©rifier la qualitÃ© du code
make lint format

# Construire les images
make build
```

### Contribution

1. Fork le projet
2. CrÃ©er une branche feature (`git checkout -b feature/amazing-feature`)
3. Commit les changements (`git commit -m 'Add amazing feature'`)
4. Push vers la branche (`git push origin feature/amazing-feature`)
5. Ouvrir une Pull Request

## Troubleshooting

### ProblÃ¨mes frÃ©quents

**1. API ne dÃ©marre pas**
```bash
# VÃ©rifier les logs
make logs-api

# VÃ©rifier la connectivitÃ© Ollama
curl http://localhost:11434/api/tags
```

**2. Vector store vide**
```bash
# IngÃ©rer des documents d'exemple
make ingest-sample

# VÃ©rifier les stats
curl http://localhost:8000/api/v1/vector-store/stats
```

**3. Performance dÃ©gradÃ©e**
```bash
# VÃ©rifier les mÃ©triques
make monitor

# Nettoyer le cache
docker exec mar-redis redis-cli FLUSHALL
```

### Support

- **Documentation** : http://docs.mar-platform.com
- **Issues GitHub** : https://github.com/your-org/mar-platform/issues
- **Discussions** : https://github.com/your-org/mar-platform/discussions
- **Email** : support@mar-platform.com

## Licence

Ce projet est sous licence MIT. Voir [LICENSE](LICENSE) pour plus de dÃ©tails.

---

**ðŸ¤– Plateforme MAR - Intelligence artificielle locale et observable**
